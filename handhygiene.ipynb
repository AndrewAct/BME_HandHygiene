{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hand Hygigene Project \n## Target: Deploy CV2, TF, and Keras to detect hand wash process \n\n## Goal: \n1. Distinguish the current step\n2. Remind the operator if she/he doesn't follow the correct procedure"},{"metadata":{},"cell_type":"markdown","source":"## 0.0 Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2 \nimport math\nimport numpy as np \nimport matplotlib.pyplot as plt \n%matplotlib inline\nfrom kaggle_datasets import KaggleDatasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras \nfrom keras.preprocessing import image\nfrom keras.utils import np_utils\nfrom skimage.transform import resize\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0.1 Explore Sample Video"},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nvideoSample1 = \"../input/hand-wash-dataset/HandWashDataset/HandWashDataset/Step_1/HandWash_001_A_01_G01.mp4\"\ncap = cv2.VideoCapture(videoSample1)\nframeRate = cap.get(5)\n\nx = 1 \nwhile (cap.isOpened()):\n    frameID = cap.get(1)\n    ret, frame = cap.read()\n    if (ret != True):\n        break\n    if (frameID % math.floor(frameRate) == 0):\n        filename = \"frame%d.jpg\" % count; count += 1 \n        cv2.imwrite(filename, frame)\n        \ncap.release()\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Try it out"},{"metadata":{"trusted":true},"cell_type":"code","source":"img = plt.imread('frame0.jpg')\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0.2 Load the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/hand-wash-dataset/HandWashDataset/HandWashDataset\"\nstep1 = \"../input/hand-wash-dataset/HandWashDataset/HandWashDataset/Step_1\"\nfor file in step1:\n    count = 0\n    cap = cv2.VideoCapture(file)\n    frameRate = cap.get(5)\n    \n    x = 1 \n    while (cap.isOpened()):\n        frameID = cap.get(1)\n        ret, frame = cap.read()\n        if (ret != True):\n            break\n        if (frameID % math.floor(frameRate) == 0):\n            filename = \"Step_1_frame%d.jpg\" % count; count += 1\n            cv2.imwrite(filename, frame)\n            \ncap.release()\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(25):\n    image = \"Step_1_frame%d.jpg\" % i\n    try:\n        img = plt.imread(image)\n        plt.imshow(img)\n    except OSError as e:\n        pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Do the same thing for Step 1 to Step 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import walk\nfor (dirpath, dirnames, filenames) in walk(\"../input\"):\n    print(\"Directory path: \", dirpath)\n    print(\"Folder name: \", dirnames)\n    print(\"File name: \", filenames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#(dirpath, dirnames, filenames) = walk(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"steps = [[\"Step 1\", 0], [\"Step 2\",0],[\"Step 3\",0],[\"Step 4\",0],[\"Step 5\",0],[\"Step 6\",0],[\"Step 7\",0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nfor file in filenames:\n    video = file\n    cap = cv2.VideoCapture(video)\n    frameRate = cap.get(5)\n    x = 1 \n    while (cap.isOpened()):\n        frameID = cap.get(1)\n        ret, frame = cap.read()\n        if (ret != True):\n            break\n        if (frameID % math.floor(frameRate) == 0):\n            filename = \"frame%d.jpg\" % count; count += 1\n            cv2.imwrite(filename, frame)\n            \n    cap.release()\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### A low-efficient method to convert all video to images"},{"metadata":{},"cell_type":"markdown","source":"## 0.2 Hand Recognition\n\n*Note: the following code referred to this repo:*\n*https://github.com/BlueDi/Hand-Detection/blob/master/video_detection.py*"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef sense(avg_color, newH, newS, newV, maxSensibility):\n    hSens = (newH * maxSensibility[0]) / 100\n    SSens = (newS * maxSensibility[1]) / 100\n    VSens = (newV * maxSensibility[2]) / 100\n    lower_bound_color = np.array(\n        [avg_color[0] - hSens, avg_color[1] - SSens, avg_color[2] - VSens])\n    upper_bound_color = np.array(\n        [avg_color[0] + hSens, avg_color[1] + SSens, avg_color[2] + VSens])\n    return np.array([lower_bound_color, upper_bound_color])\n\n\ndef start(avg_color, max_sensibility, video=True, path=None, left=False):\n    hSensibility = 100\n    sSensibility = 100\n    vSensibility = 100\n\n    apply_sensibility(avg_color, hSensibility, sSensibility, vSensibility,\n                      max_sensibility)\n\n    cv2.namedWindow('Hand Detection')\n    cv2.createTrackbar('HSensb', 'Hand Detection', hSensibility, 100, nothing)\n    cv2.createTrackbar('SSensb', 'Hand Detection', sSensibility, 100, nothing)\n    cv2.createTrackbar('VSensb', 'Hand Detection', vSensibility, 100, nothing)\n\n    if path != None:\n        frame = cv2.imread(path)\n        hand_detection(frame, lower_bound_color, upper_bound_color, left)\n        cv2.waitKey(0)\n    else:\n        video_capture = cv2.VideoCapture(0)\n\n        while True:\n            try:\n                _, frame = video_capture.read()\n                frame = cv2.flip(frame, 1)\n\n                # get values from trackbar\n                newHSens = cv2.getTrackbarPos('HSensb', 'Hand Detection')\n                newSSens = cv2.getTrackbarPos('SSensb', 'Hand Detection')\n                newVSens = cv2.getTrackbarPos('VSensb', 'Hand Detection')\n\n                # and apply the new sensibility values\n                lower_bound_color, upper_bound_color = apply_sensibility(\n                    avg_color, newHSens, newSSens, newVSens, max_sensibility)\n\n                hand_detection(frame, lower_bound_color, upper_bound_color,\n                               left)\n\n            except Exception as e:\n                print e\n                pass\n\n            if not video:\n                cv2.waitKey(0)\n                cv2.destroyAllWindows()\n                break\n\n            key = cv2.waitKey(10)\n            if key == ord('q'):\n                video_capture.release()\n                cv2.destroyAllWindows()\n                break\n\n\ndef hand_detection(frame, lower_bound_color, upper_bound_color, left):\n    \n    kernel = np.ones((3, 3), np.uint8)\n\n    if left:\n        roi = frame[100:300, 100:300]\n        cv2.rectangle(frame, (100, 100), (300, 300), (0, 255, 0), 0)\n    else:\n        roi = frame[50:300, 300:550]\n        cv2.rectangle(frame, (300, 50), (550, 300), (0, 255, 0), 0)\n\n    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n\n    binary_mask = cv2.inRange(hsv, lower_bound_color, upper_bound_color)\n    mask = cv2.dilate(binary_mask, kernel, iterations=3)\n    mask = cv2.erode(mask, kernel, iterations=3)\n    mask = cv2.GaussianBlur(mask, (5, 5), 90)\n\n    _, contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE,\n                                              cv2.CHAIN_APPROX_SIMPLE)\n    try:\n        cnt = max(contours, key=lambda x: cv2.contourArea(x))\n        l = analyse_defects(cnt, roi)\n        analyse_contours(frame, cnt, l + 1)\n    except ValueError:\n        pass\n\n    show_results(binary_mask, mask, frame)\n\n\ndef analyse_defects(cnt, roi):\n    \n    epsilon = 0.0005 * cv2.arcLength(cnt, True)\n    approx = cv2.approxPolyDP(cnt, epsilon, True)\n\n    hull = cv2.convexHull(approx, returnPoints=False)\n    defects = cv2.convexityDefects(approx, hull)\n\n    l = 0\n    if defects is not None:\n        for i in range(defects.shape[0]):\n            s, e, f, d = defects[i, 0]\n            start = tuple(approx[s][0])\n            end = tuple(approx[e][0])\n            far = tuple(approx[f][0])\n            pt = (100, 180)\n\n            a = math.sqrt((end[0] - start[0])**2 + (end[1] - start[1])**2)\n            b = math.sqrt((far[0] - start[0])**2 + (far[1] - start[1])**2)\n            c = math.sqrt((end[0] - far[0])**2 + (end[1] - far[1])**2)\n            s = (a + b + c) / 2\n            ar = math.sqrt(s * (s - a) * (s - b) * (s - c))\n\n            d = (2 * ar) / a\n\n            angle = math.acos((b**2 + c**2 - a**2) / (2 * b * c)) * 57\n\n            if angle <= 90 and d > 30:\n                l += 1\n                cv2.circle(roi, far, 3, [255, 0, 0], -1)\n            cv2.line(roi, start, end, [0, 255, 0], 2)\n    return l\n\n\ndef analyse_contours(frame, cnt, l):\n    hull = cv2.convexHull(cnt)\n\n    areahull = cv2.contourArea(hull)\n    areacnt = cv2.contourArea(cnt)\n\n    arearatio = ((areahull - areacnt) / areacnt) * 100\n\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    if l == 1:\n        if areacnt < 2000:\n            cv2.putText(frame, 'Put hand in the box', (0, 50), font, 2,\n                        (0, 0, 255), 3, cv2.LINE_AA)\n        else:\n            if arearatio < 12:\n                cv2.putText(frame, '0', (0, 50), font, 2, (0, 0, 255), 3,\n                            cv2.LINE_AA)\n            elif arearatio < 17.5:\n                cv2.putText(frame, 'Fixe', (0, 50), font, 2, (0, 0, 255), 3,\n                            cv2.LINE_AA)\n            else:\n                cv2.putText(frame, '1', (0, 50), font, 2, (0, 0, 255), 3,\n                            cv2.LINE_AA)\n    elif l == 2:\n        cv2.putText(frame, '2', (0, 50), font, 2, (0, 0, 255), 3, cv2.LINE_AA)\n    elif l == 3:\n        if arearatio < 27:\n            cv2.putText(frame, '3', (0, 50), font, 2, (0, 0, 255), 3,\n                        cv2.LINE_AA)\n        else:\n            cv2.putText(frame, 'ok', (0, 50), font, 2, (0, 0, 255), 3,\n                        cv2.LINE_AA)\n    elif l == 4:\n        cv2.putText(frame, '4', (0, 50), font, 2, (0, 0, 255), 3, cv2.LINE_AA)\n    elif l == 5:\n        cv2.putText(frame, '5', (0, 50), font, 2, (0, 0, 255), 3, cv2.LINE_AA)\n    elif l == 6:\n        cv2.putText(frame, 'reposition', (0, 50), font, 2, (0, 0, 255), 3,\n                    cv2.LINE_AA)\n    else:\n        cv2.putText(frame, 'reposition', (10, 50), font, 2, (0, 0, 255), 3,\n                    cv2.LINE_AA)\n\n\ndef show_results(binary_mask, mask, frame):\n    combine_masks = np.concatenate((binary_mask, mask), axis=0)\n    height, _, _ = frame.shape\n    _, width = combine_masks.shape\n    masks_result = cv2.resize(combine_masks, dsize=(width, height))\n    masks_result = cv2.cvtColor(masks_result, cv2.COLOR_GRAY2BGR)\n    result_image = np.concatenate((frame, masks_result), axis=1)\n    cv2.imshow('Hand Detection', result_image)\n\n\ndef main():\n    lower_color = np.array([0, 50, 120], dtype=np.uint8)\n    upper_color = np.array([180, 150, 250], dtype=np.uint8)\n\n    start(lower_color, upper_color)\n\n\nif __name__ == '__main__':\n    main()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}